seed_everything: 1990
trainer:
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir:  /home/ubuntu/fomo25_challenge/pipelines/pretrain_mim_med3d/lightning/logs/
      name: fomo_seg
      version: unetr_base_vitmae_p16_m0.5_full_shallow_continued3_overlap0.5
      default_hp_metric: false
  # checkpoint_callback: null
  enable_checkpointing: true
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: /home/ubuntu/fomo25_challenge/pipelines/pretrain_mim_med3d/lightning/checkpoints/unetr_base_vitmae_p16_m0.5_full_shallow_continued3_overlap0.5
        filename: epoch{epoch:02d}_step{step:04d}_val_dice{val/dice_score_avg:.5f}
        monitor: val/dice_score_avg
        mode: max
        save_weights_only: true
        auto_insert_metric_name: false
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  default_root_dir: /home/ubuntu/fomo25_challenge/pipelines/pretrain_mim_med3d/lightning/results/unetr_base_vitmae_p16_m0.5_full_shallow_continued3_overlap0.5
  gradient_clip_val: null
  gradient_clip_algorithm: null
  # process_position: 0  # Deprecated in v2.x
  num_nodes: 1
  devices: -1
  accelerator: gpu
  # auto_select_gpus: false  # Deprecated in v2.x
  # tpu_cores: null  # Not used in this setup
  # ipus: null  # Not used in this setup
  # log_gpu_memory: null  # Deprecated in v2.x
  # progress_bar_refresh_rate: null  # Replaced by log_every_n_steps
  enable_progress_bar: true
  overfit_batches: 0.0
  # track_grad_norm: -1
  check_val_every_n_epoch: 10
  fast_dev_run: false
  accumulate_grad_batches: 1 
  # max_epochs: 10
  max_epochs: 2000
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  limit_predict_batches: 1.0
  val_check_interval: 1.0
  # flush_logs_every_n_steps: null  # Deprecated in v2.x
  log_every_n_steps: 5
  strategy: auto
  sync_batchnorm: false
  precision: 16-mixed
  enable_model_summary: true
  # weights_summary: top  # Replaced by enable_model_summary
  # weights_save_path: null  # Deprecated in v2.x
  num_sanity_val_steps: 2
  # resume_from_checkpoint: null  # Replaced by ckpt_path at root level
  profiler: null
  benchmark: true
  deterministic: false
  reload_dataloaders_every_n_epochs: 0
  # reload_dataloaders_every_epoch: false  # Replaced by reload_dataloaders_every_n_epochs
  # auto_lr_find: false  # Moved to separate command in v2.x
  # replace_sampler_ddp: false  # Automatic in v2.x
  detect_anomaly: false
  # auto_scale_batch_size: false
  # prepare_data_per_node: null  # Deprecated in v2.x
  plugins: null
  # amp_backend: native
  # amp_level: null  # Not used with native amp
  # move_metrics_to_cpu: false
  # multiple_trainloader_mode: max_size_cycle
  # stochastic_weight_avg: false  # Moved to callbacks in v2.x
model:
  class_path: __main__.SingleSegtrainer
  init_args:
    num_classes: 2
    model_name: unetr_base
    model_dict:
      pretrained: /home/ubuntu/fomo25_challenge/pipelines/pretrain_mim_med3d/lightning/checkpoints/vitsimmim_base_p16_m0.5_full_shallow_0.5cycle_lr0.00001_seed1990_continued3/epoch19_step0000_val_loss0.06937.ckpt
      in_channels: 1
      out_channels: 2
      img_size: [96, 96, 16]
      feature_size: 16
      hidden_size: 1024
      mlp_dim: 4096  # MLP dimension in transformer blocks (4x hidden_size)
      num_layers: 8  # Number of transformer layers
      num_heads: 16  # Number of attention heads in transformer blocks
      pos_embed: perceptron
      norm_name: instance
      res_block: true
      conv_block: true
      dropout_rate: 0.0
      revise_keys: [["model.encoder.", "vit."]]
    save_examples: 3
    gradual_unfreeze: True
    # progressive_unfreezing_schedule: [3,6,9]
    progressive_unfreezing_schedule: [10, 200, 500]
    # Learning rate configuration
    backbone_lr: 1.0e-5              # Learning rate for pretrained ViT backbone
    backbone_weight_decay: 1.0e-5    # Weight decay for backbone parameters
    decoder_lr: 1.0e-4               # Learning rate for decoder/head parameters  
    decoder_weight_decay: 1.0e-4     # Weight decay for decoder parameters
    # Scheduler configuration
    scheduler_T_0: 100                 # CosineAnnealingWarmRestarts - restart every N epochs
    scheduler_T_mult: 2               # CosineAnnealingWarmRestarts - multiply restart period
    scheduler_eta_min: 1.0e-6         # CosineAnnealingWarmRestarts - minimum learning rate
    lambda_dice: 0.8
    lambda_ce: 0.2
    class_weights: [0.1, 0.9]
    sliding_window_overlap: 0.5
data:
  class_path: data.seg_dataset.FomoSegDataset
  init_args:
    # root_dir: /home/ubuntu/fomo25_challenge/pipelines/pretrain_mim_med3d/lightning/data/fomo_seg/
    train_json_path: /home/ubuntu/fomo25_challenge/data/metadata/preprocessed/Task002_FOMO2/task2_seg_train_file_list_t2flair.json
    val_json_path: /home/ubuntu/fomo25_challenge/data/metadata/preprocessed/Task002_FOMO2/task2_seg_val_file_list_t2flair.json
    cache_dir: /home/ubuntu/fomo25_challenge/data/cache/
    # modality: t1
    batch_size: 4
    val_batch_size: 1
    num_workers: 192
    cache_num: 1001
    cache_rate: 1.0
    spatial_size: [96, 96, 16]
    num_samples: 4
    dist: true
    # Progressive augmentation thresholds
    light_to_medium_epoch: 200    # Switch to medium augs 
    medium_to_full_epoch: 500     # Switch to full augs 
    # sw_batch_size: 4
ckpt_path: null
